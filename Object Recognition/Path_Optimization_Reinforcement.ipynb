{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0       161\n",
      "1       114\n",
      "2        70\n",
      "3       160\n",
      "4       113\n",
      "5        69\n",
      "6       153\n",
      "7       106\n",
      "8        62\n",
      "9       143\n",
      "10       96\n",
      "11       52\n",
      "12      152\n",
      "13      105\n",
      "14       61\n",
      "15      172\n",
      "16      125\n",
      "17       81\n",
      "18      176\n",
      "19      129\n",
      "20       85\n",
      "21      169\n",
      "22      122\n",
      "23       78\n",
      "24      165\n",
      "25      118\n",
      "26       74\n",
      "27      165\n",
      "28      118\n",
      "29       74\n",
      "...     ...\n",
      "151305  182\n",
      "151306  119\n",
      "151307   65\n",
      "151308  182\n",
      "151309  117\n",
      "151310   61\n",
      "151311  186\n",
      "151312  121\n",
      "151313   63\n",
      "151314  188\n",
      "151315  122\n",
      "151316   62\n",
      "151317  176\n",
      "151318  110\n",
      "151319   50\n",
      "151320  190\n",
      "151321  126\n",
      "151322   65\n",
      "151323  186\n",
      "151324  122\n",
      "151325   61\n",
      "151326  179\n",
      "151327  117\n",
      "151328   58\n",
      "151329  171\n",
      "151330  112\n",
      "151331   54\n",
      "151332  164\n",
      "151333  108\n",
      "151334   51\n",
      "\n",
      "[151335 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#source: https://www.hackerearth.com/practice/notes/extracting-pixel-values-of-an-image-in-python/\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "im = Image.open('rocks.jpg', 'r')\n",
    "pix_val = list(im.getdata())\n",
    "pix_val_flat = [x for sets in pix_val for x in sets]\n",
    "#print(pix_val_flat)\n",
    "\n",
    "pix_val_pand = pd.DataFrame(pix_val_flat)\n",
    "print(pix_val_pand)\n",
    "#makes rows of lists by length and width\n",
    "\n",
    "#make a flowchart explaining how the program will detect obstacles and send coordinates to the python intepreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-799bfdd8b841>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-799bfdd8b841>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    if(i+1<=mazeRow) if(maze(i+1,j)==0 and maze(i,j)==0) # plain plain rew = -1; end if(maze(i+1,j)==0 and maze(i,j)==1) % hill plain rew = -1; end if(maze(i+1,j)==1 && maze(i,j)==0) % plain hill rew = -3; end if(maze(i+1,j)==1 && maze(i,j)==1) % hill-hill rew = -2; end if maze(i+1,j)==goalmark rewardMat(stateIterator,actDown)= rewforGoal; %10 else rewardMat(stateIterator,actDown)= rew; end transitMat(stateIterator,actDown) = stateIterator+(abs(mazeColumn-j)+abs(1-j)+1);  %Left if(j-1>0)\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#source: https://dzone.com/articles/optimal-path-detection-with-reinforcement-learning\n",
    "\n",
    "rewardMat =  zeros(mazeRow*mazeColumn,length(ACTIONS))\n",
    "transitMat = zeros(mazeRow*mazeColumn,length(ACTIONS))\n",
    "stateIterator=0\n",
    "rew = 0\n",
    "rewforGoal =10\n",
    "for i in mazeRow:\n",
    "    for j in mazeColumn:\n",
    "        stateIterator = (i-1)* mazeColumn + j\n",
    "        if(i-1>=1):\n",
    "            if(maze(i-1,j)==0 and maze(i,j)==0): \n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i-1,j)==0 and maze(i,j)==1):  \n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i-1,j)==1 and maze(i,j)==0):  \n",
    "                rew = -3;\n",
    "            \n",
    "            if(maze(i-1,j)==1 and maze(i,j)==1):  \n",
    "                rew = -2;\n",
    "            \n",
    "            if maze(i-1,j)==goalmark:\n",
    "                rewardMat(stateIterator,actUp)= rewforGoal; \n",
    "            else:\n",
    "                rewardMat(stateIterator,actUp)= rew;\n",
    "            \n",
    "            transitMat(stateIterator,actUp) = stateIterator-(abs(mazeColumn-j)+abs(1-j)+1);\n",
    "                \n",
    "\n",
    "        #check if statement\n",
    "        if(i+1<=mazeRow) if(maze(i+1,j)==0 and maze(i,j)==0) # plain plain rew = -1; if(maze(i+1,j)==0 and maze(i,j)==1) % hill plain rew = -1; end if(maze(i+1,j)==1 && maze(i,j)==0) % plain hill rew = -3; end if(maze(i+1,j)==1 && maze(i,j)==1) % hill-hill rew = -2; end if maze(i+1,j)==goalmark rewardMat(stateIterator,actDown)= rewforGoal; %10 else rewardMat(stateIterator,actDown)= rew; end transitMat(stateIterator,actDown) = stateIterator+(abs(mazeColumn-j)+abs(1-j)+1);  %Left if(j-1>0)\n",
    "            if(maze(i,j-1)==0 and maze(i,j)==0): # plain plain\n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i,j-1)==0 and maze(i,j)==1): # hill plain\n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i,j-1)==1 and maze(i,j)==0): # plain hill\n",
    "                rew = -3;\n",
    "            \n",
    "            if(maze(i,j-1)==1 and maze(i,j)==1): # hill-hill\n",
    "                rew = -2;\n",
    "            \n",
    "            if maze(i,j-1)==goalmark:\n",
    "                rewardMat(stateIterator,actLeft)= rewforGoal;  \n",
    "            else:\n",
    "                rewardMat(stateIterator,actLeft)= rew;\n",
    "            \n",
    "            transitMat(stateIterator,actLeft) = stateIterator-1;\n",
    "        \n",
    "       \n",
    "        if(j+1<=mazeColumn)\n",
    "            if(maze(i,j+1)==0 and maze(i,j)==0) # plain plain\n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i,j+1)==0 and maze(i,j)==1) # hill plain\n",
    "                rew = -1;\n",
    "            \n",
    "            if(maze(i,j+1)==1 and maze(i,j)==0) # plain hill\n",
    "                rew = -3;\n",
    "            \n",
    "            if(maze(i,j+1)==1 and maze(i,j)==1) # hill-hill\n",
    "                rew = -2;\n",
    "            \n",
    "            if maze(i,j+1)==goalmark\n",
    "                rewardMat(stateIterator,actRight)= rewforGoal;  \n",
    "            else\n",
    "                rewardMat(stateIterator,actRight)= rew;\n",
    "            \n",
    "            transitMat(stateIterator,actRight) = stateIterator+1;\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://dzone.com/articles/optimal-path-detection-with-reinforcement-learning\n",
    "\n",
    "#Finding Goal and Start states positions\n",
    "[sx,sy] = find(maze==startmark)\n",
    "startState = (sx-1)*mazeColumn+sy\n",
    "[gx,gy] = find(maze==goalmark)\n",
    "goalState = (gx-1)*mazeColumn+gy\n",
    "\n",
    "#Initializing Parameteters\n",
    "Q = transitMat*0\n",
    "Q(goalState,1) = 10 \n",
    "gamma = 1\n",
    "alpha = 0.5\n",
    "randomProb = 0.25\n",
    "tx = sx\n",
    "ty = sy\n",
    "V = zeros(mazeColumn*mazeRow,1)\n",
    "episode = 30000\n",
    "stepSize = 20\n",
    "rewforGoal = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://dzone.com/articles/optimal-path-detection-with-reinforcement-learning\n",
    "#Start Q Learning\n",
    "for it in episode:\n",
    "        \n",
    "    tx = sx\n",
    "    ty = sy\n",
    "    next = 0\n",
    "    \n",
    "    for step in stepSize:\n",
    "            \n",
    "        next = 0\n",
    "        \n",
    "        #Defining Current Position and fing possible actions\n",
    "        curr =  (tx-1)*mazeColumn+ty\n",
    "        actionOptions = Q(curr,:)\n",
    "        \n",
    "        #Finding Next Action with using random probability\n",
    "        while next==0:\n",
    "            #choosing random action\n",
    "            if rand < randomProb:\n",
    "                randAction = randperm(length(actionOptions))\n",
    "                index = randAction(1)\n",
    "            else:\n",
    "                [value, index] = max(actionOptions)\n",
    "            next = transitMat(curr, index)\n",
    "        \n",
    "        # Get reward for current position\n",
    "        rew = rewardMat(curr,index)\n",
    "       \n",
    "        # Q Learning Equation\n",
    "        if(curr~=goalState):\n",
    "            Q(curr,index) = Q(curr,index) + alpha*( (rew+gamma*max(Q(next,:))) - (Q(curr,index)))\n",
    "        \n",
    "        # Finding Next Step Position\n",
    "        sect = fix((next/mazeColumn))\n",
    "        if sect == 0:\n",
    "            sect =1\n",
    "        \n",
    "        rest = mod(next,mazeColumn)\n",
    "        if next==mazeColumn*mazeRow:\n",
    "            tx = sect\n",
    "            ty = mazeColumn\n",
    "        else:\n",
    "            if ty == 0:\n",
    "                tx = sect\n",
    "                ty = mazeColumn\n",
    "            else:\n",
    "                tx = sect+1\n",
    "                ty = rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://dzone.com/articles/optimal-path-detection-with-reinforcement-learning\n",
    "#get the figure and axes handles\n",
    "\n",
    "hFig = gcf\n",
    "hAx = gca\n",
    "#set the figure to full screen\n",
    "set(hFig,'units','normalized','outerposition',[0 0 1 1])\n",
    "\n",
    "#Plot V Values using Q Matrix\n",
    "subplot(1,3,1)\n",
    "V = max(Q,[],2)\n",
    "V = reshape(V,mazeRow,mazeColumn)\n",
    "\n",
    "\n",
    "#V(goalState) = 10;\n",
    "imagesc(V)\n",
    "title('V Values')\n",
    "#hold on;\n",
    "for i in mazeRow:\n",
    "    for j in mazeColumn:\n",
    "        cell = (i-1)*mazeColumn+j\n",
    "        content = sprintf('%2.2f',V(cell))\n",
    "        labels = text(j-0.20,i, content)\n",
    "        set(labels)\n",
    "   \n",
    "    #Plot Q Values\n",
    "subplot(1,3,2)\n",
    "imagesc(maze)\n",
    "title('Q Values')\n",
    "\n",
    "#hold on;\n",
    "for i in mazeRow:\n",
    "    for j in mazeColumn:\n",
    "        cell = (i-1)*mazeColumn+j\n",
    "        label1 = text(j-0.15,i-0.21, sprintf('%2.2f',Q(cell,actUp))) \n",
    "        set(label1,'FontSize',8)\n",
    "        label2 = text(j-0.15,i+0.22, sprintf('%2.2f',Q(cell,actDown)))\n",
    "        set(label2,'FontSize',8)\n",
    "        label3 = text(j-0.40,i, sprintf('%2.2f',Q(cell,actLeft))) \n",
    "        set(label3,'FontSize',8)\n",
    "        label4 = text(j+0.16,i, sprintf('%2.2f',Q(cell,actRight))) \n",
    "        set(label4,'FontSize',8)       \n",
    "\n",
    "\n",
    "#Plot Optimal Path\n",
    "subplot(1,3,3)\n",
    "imagesc(maze)\n",
    "title('Optimal Path')\n",
    "#hold on;\n",
    "curr = startState\n",
    "tx = sx\n",
    "ty = sy\n",
    "QPr = Q\n",
    "QPr(QPr==0)=-1000\n",
    "while curr~=goalState:\n",
    "    \n",
    "    curr =  (tx-1)*mazeColumn+ty\n",
    "    \n",
    "    if curr==startState:\n",
    "        labelN = text(ty-0.40,tx-0.4, sprintf('START'))\n",
    "        set(labelN,'FontSize',15)\n",
    "    elif curr==goalState:\n",
    "        labelN = text(ty-0.35,tx, sprintf('GOAL'))\n",
    "        set(labelN,'FontSize',15)\n",
    "        continue;\n",
    "        \n",
    "    actionOptions = QPr(curr,:)   \n",
    "    [value, index] = max(actionOptions)\n",
    "    \n",
    "    if index == actLeft:\n",
    "        labelN = text(ty-0.2,tx, sprintf('L'))\n",
    "        set(labelN,'FontSize',25)\n",
    "        ty = ty-1\n",
    "\n",
    "    if index == actRight:\n",
    "        labelN = text(ty-0.2,tx, sprintf('R'))\n",
    "        set(labelN,'FontSize',25)\n",
    "        ty = ty+1\n",
    "    \n",
    "    if index == actUp:\n",
    "        labelN = text(ty-0.2,tx, sprintf('U'))\n",
    "        set(labelN,'FontSize',25)\n",
    "        tx = tx -1;\n",
    "    \n",
    "    if index == actDown:\n",
    "        labelN = text(ty-0.2,tx, sprintf('D'))\n",
    "        set(labelN,'FontSize',25)\n",
    "        tx = tx +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
